{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrYZPC/G2eKFOFR9qyrwBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NurFortuna/Deep_Learning_with_Tensorflow_notes/blob/main/Deep_Learning_applied_on_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcwsQffJ-Kcy",
        "outputId": "b8d9e3f9-0e3c-4328-df53-e1a2791cd8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting grpcio==1.24.3\n",
            "  Downloading grpcio-1.24.3-cp38-cp38-manylinux2010_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from grpcio==1.24.3) (1.15.0)\n",
            "Installing collected packages: grpcio\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.1\n",
            "    Uninstalling grpcio-1.51.1:\n",
            "      Successfully uninstalled grpcio-1.51.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.24.3 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.24.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.14.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (0.38.4)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.21.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.3.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (3.19.6)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, scipy, h5py, gast, cachetools, google-auth, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.1\n",
            "    Uninstalling cachetools-5.2.1:\n",
            "      Successfully uninstalled cachetools-5.2.1\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.0\n",
            "    Uninstalling google-auth-2.16.0:\n",
            "      Successfully uninstalled google-auth-2.16.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.4.0 requires scipy>=1.6, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.24.3 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install grpcio==1.24.3\n",
        "!pip install tensorflow==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "DNcj0qEU-SxG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "0hlBO4Ts-VIt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "KhUu-esR_bbv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"categorical labels\")\n",
        "print(y_train[0:5])\n",
        "\n",
        "# make labels one hot encoded\n",
        "y_train = tf.one_hot(y_train, 10)\n",
        "y_test = tf.one_hot(y_test, 10)\n",
        "\n",
        "print(\"one hot encoded labels\")\n",
        "print(y_train[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MpKgrbf_en6",
        "outputId": "a53fb173-7097-4c69-eb86-e8c488059ccd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "categorical labels\n",
            "[5 0 4 1 9]\n",
            "one hot encoded labels\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(5, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial parameters\n",
        "width = 28 # width of the image in pixels \n",
        "height = 28 # height of the image in pixels\n",
        "flat = width * height # number of pixels in one image \n",
        "class_output = 10 # number of possible classifications for the problem"
      ],
      "metadata": {
        "id": "1LLViPxV-XDB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting images of the data set to tensors\n",
        "x_image_train = tf.reshape(x_train, [-1,28,28,1])  \n",
        "x_image_train = tf.cast(x_image_train, 'float32') \n",
        "\n",
        "x_image_test = tf.reshape(x_test, [-1,28,28,1]) \n",
        "x_image_test = tf.cast(x_image_test, 'float32') \n",
        "\n",
        "#creating new dataset with reshaped inputs\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_image_train, y_train)).batch(50)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_image_test, y_test)).batch(50)"
      ],
      "metadata": {
        "id": "pHan2tSt-sz0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_image_train = tf.slice(x_image_train,[0,0,0,0],[10000, 28, 28, 1])\n",
        "y_train = tf.slice(y_train,[0,0],[10000, 10])"
      ],
      "metadata": {
        "id": "XOdlLaqd_CUw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layer 1 ###\n",
        "\n",
        "We define a kernel here. The Size of the filter/kernel is 5x5; Input channels is 1 (grayscale); and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 28x28x32). In this step, we create a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]"
      ],
      "metadata": {
        "id": "CDMDSERu_mI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev=0.1, seed=0))\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs"
      ],
      "metadata": {
        "id": "ErOyETFv_Pye"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size,28 ,28, 1]\n",
        "\n",
        "a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. W is of size [5, 5, 1, 32]\n",
        "\n",
        "stride which is [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions: [batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. strides determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. [1, 2, 2, 1]"
      ],
      "metadata": {
        "id": "P6hwf5OYBSM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process:\n",
        "\n",
        "Change the filter to a 2-D matrix with shape [5*5*1,32]\n",
        "\n",
        "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, 28, 28, 5*5*1].\n",
        "\n",
        "For each batch, right-multiplies the filter matrix and the image vector."
      ],
      "metadata": {
        "id": "ow0nMA03BwIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve1(x):\n",
        "    return(\n",
        "        tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)"
      ],
      "metadata": {
        "id": "FiKPEtF5BCkV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![resim](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)"
      ],
      "metadata": {
        "id": "ODZ-GXiNDX-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the ReLU activation Function\n",
        "def h_conv1(x): return(tf.nn.relu(convolve1(x)))"
      ],
      "metadata": {
        "id": "lXFYTGBjCFqs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Apply the max pooling ###\n",
        " It partitions the input image into a set of rectangles and, and then find the maximum value for that region.\n",
        "\n",
        " Lets use tf.nn.max_pool function to perform max pooling. Kernel size: 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)\n",
        "\n",
        " Strides: dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping. The input is a matrix of size 28x28x32, and the output would be a matrix of size 14x14x32.\n",
        "\n",
        " ![resim](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)"
      ],
      "metadata": {
        "id": "U0QoQmIeDwqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv1(x):\n",
        "    return tf.nn.max_pool(h_conv1(x), ksize=[1, 2, 2, 1], \n",
        "                          strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "metadata": {
        "id": "RLZ3L0MyEP02"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layer 2 ###"
      ],
      "metadata": {
        "id": "BAqxfqGtEzLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter/kernel: 5x5 (25 pixels)\n",
        "\n",
        "Input channels: 32 (from the 1st Conv layer, we had 32 feature maps)\n",
        "\n",
        "64 output feature maps"
      ],
      "metadata": {
        "id": "hubqalD8E61C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=1))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs"
      ],
      "metadata": {
        "id": "eoYtWNqNENst"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convolve image with weight tensor and add biases.\n",
        "def convolve2(x): \n",
        "    return( \n",
        "    tf.nn.conv2d(conv1(x), W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)"
      ],
      "metadata": {
        "id": "u9IrWY4xFLSO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the ReLU activation Function\n",
        "def h_conv2(x):  return tf.nn.relu(convolve2(x))"
      ],
      "metadata": {
        "id": "YS0mxcbJFhpa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the max pooling\n",
        "def conv2(x):  \n",
        "    return(\n",
        "    tf.nn.max_pool(h_conv2(x), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'))"
      ],
      "metadata": {
        "id": "i_tZf-sCFmzI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fully Connected Layer ####\n",
        "\n",
        "You need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.\n",
        "\n",
        "So, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024]\n"
      ],
      "metadata": {
        "id": "UODLTTrKGnr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Flattening Second Layer\n",
        "def layer2_matrix(x): return tf.reshape(conv2(x), [-1, 7 * 7 * 64])"
      ],
      "metadata": {
        "id": "F5G63c35Ggp_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Weights and Biases between layer 2 and 3\n",
        "W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1, seed = 2))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # need 1024 biases for 1024 outputs"
      ],
      "metadata": {
        "id": "Yi8qhhD9Hv5D"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matrix Multiplication (applying weights and biases)\n",
        "def fcl(x): return tf.matmul(layer2_matrix(x), W_fc1) + b_fc1"
      ],
      "metadata": {
        "id": "v8pB1ptxH_8y"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the ReLU activation Function\n",
        "def h_fc1(x): return tf.nn.relu(fcl(x))"
      ],
      "metadata": {
        "id": "2n1D_CBgIHC8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropout Layer, Optional phase for reducing overfitting ####\n",
        "\n",
        "It is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes. This can be very useful for very large neural networks to prevent overfitting."
      ],
      "metadata": {
        "id": "Ns-c9B4KITwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_prob=0.5\n",
        "def layer_drop(x): return tf.nn.dropout(h_fc1(x), keep_prob)"
      ],
      "metadata": {
        "id": "dAoCWI5eIQba"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax Layer\n",
        "#Input channels: 1024 (neurons from the 3rd Layer); 10 output features"
      ],
      "metadata": {
        "id": "eUo-pKEQIs2e"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1, seed = 2)) #1024 neurons\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]"
      ],
      "metadata": {
        "id": "kc-W0Bi5I1cm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matrix Multiplication (applying weights and biases)\n",
        "def fc(x): return tf.matmul(layer_drop(x), W_fc2) + b_fc2"
      ],
      "metadata": {
        "id": "wgxgRYCwUE-_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the Softmax activation Function\n",
        "def y_CNN(x): return tf.nn.softmax(fc(x))"
      ],
      "metadata": {
        "id": "CqrZGMQBUWH4"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the loss function ####"
      ],
      "metadata": {
        "id": "Pk69n7z-Vp45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_label, y_pred):\n",
        "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))"
      ],
      "metadata": {
        "id": "37ropgKQUc_K"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the optimizer ####\n",
        "\n",
        "It is obvious that we want minimize the error of our network which is calculated by cross_entropy metric. To solve the problem, we have to compute gradients for the loss (which is minimizing the cross-entropy) and apply gradients to variables. It will be done by an optimizer: GradientDescent or Adagrad."
      ],
      "metadata": {
        "id": "jf5fxyk1V7jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "P_7IJHGZVsiZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = [W_conv1, b_conv1, W_conv2, b_conv2, \n",
        "             W_fc1, b_fc1, W_fc2, b_fc2, ]\n",
        "\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = cross_entropy( y, y_CNN( x ))\n",
        "        grads = tape.gradient( current_loss , variables )\n",
        "        optimizer.apply_gradients( zip( grads , variables ) )\n",
        "        return current_loss.numpy()\n"
      ],
      "metadata": {
        "id": "7aNU71MJVtaN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define prediction\n",
        "correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1), tf.argmax(y_train, axis=1))"
      ],
      "metadata": {
        "id": "hGzOn99uVtfo"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))"
      ],
      "metadata": {
        "id": "VSJOmYjdVtkT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "loss_values=[]\n",
        "accuracies = []\n",
        "epochs = 1\n",
        "\n",
        "for i in range(epochs):\n",
        "    j=0\n",
        "    # each batch has 50 examples\n",
        "    for x_train_batch, y_train_batch in train_ds:\n",
        "        j+=1\n",
        "        current_loss = train_step(x_train_batch, y_train_batch)\n",
        "        if j%50==0: #reporting intermittent batch statistics\n",
        "            correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "            #  accuracy\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "            print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss),\n",
        "                     \"accuracy\", str(accuracy)) \n",
        "            \n",
        "    current_loss = cross_entropy( y_train, y_CNN( x_image_train )).numpy()\n",
        "    loss_values.append(current_loss)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1),\n",
        "                                  tf.argmax(y_train, axis=1))\n",
        "    #  accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "    accuracies.append(accuracy)\n",
        "    print(\"end of epoch \", str(i), \"loss\", str(current_loss), \"accuracy\", str(accuracy) )  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qw899A9XdRJ",
        "outputId": "f717b147-727e-4bc3-ebdf-b1f4bcfcfd21"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0 batch 50 loss: 93.19175 accuracy 0.54\n",
            "epoch  0 batch 100 loss: 50.721977 accuracy 0.8\n",
            "epoch  0 batch 150 loss: 28.514547 accuracy 0.78\n",
            "epoch  0 batch 200 loss: 20.925966 accuracy 0.86\n",
            "epoch  0 batch 250 loss: 31.7288 accuracy 0.84\n",
            "epoch  0 batch 300 loss: 19.00697 accuracy 0.9\n",
            "epoch  0 batch 350 loss: 30.449793 accuracy 0.84\n",
            "epoch  0 batch 400 loss: 15.62064 accuracy 0.86\n",
            "epoch  0 batch 450 loss: 23.445072 accuracy 0.84\n",
            "epoch  0 batch 500 loss: 12.635733 accuracy 0.86\n",
            "epoch  0 batch 550 loss: 7.8887844 accuracy 0.94\n",
            "epoch  0 batch 600 loss: 21.098469 accuracy 0.86\n",
            "epoch  0 batch 650 loss: 22.575092 accuracy 0.9\n",
            "epoch  0 batch 700 loss: 9.838955 accuracy 0.9\n",
            "epoch  0 batch 750 loss: 15.91292 accuracy 0.9\n",
            "epoch  0 batch 800 loss: 16.354631 accuracy 0.94\n",
            "epoch  0 batch 850 loss: 19.517063 accuracy 0.9\n",
            "epoch  0 batch 900 loss: 9.14138 accuracy 0.9\n",
            "epoch  0 batch 950 loss: 11.600172 accuracy 0.94\n",
            "epoch  0 batch 1000 loss: 7.836789 accuracy 0.92\n",
            "epoch  0 batch 1050 loss: 5.4864144 accuracy 0.96\n",
            "epoch  0 batch 1100 loss: 10.5989685 accuracy 0.88\n",
            "epoch  0 batch 1150 loss: 12.543016 accuracy 0.96\n",
            "epoch  0 batch 1200 loss: 4.103624 accuracy 0.96\n",
            "end of epoch  0 loss 1732.7513 accuracy 0.945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "j=0\n",
        "acccuracies=[]\n",
        "# evaluate accuracy by batch and average...reporting every 100th batch\n",
        "for x_train_batch, y_train_batch in train_ds:\n",
        "        j+=1\n",
        "        correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "        #accuracies.append(accuracy)\n",
        "        if j%100==0:\n",
        "            print(\"batch\", str(j), \"accuracy\", str(accuracy) ) \n",
        "import numpy as np\n",
        "print(\"accuracy of entire set\", str(np.mean(accuracies)))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu9WIlvgZKho",
        "outputId": "8a6c2ff4-fdad-4b49-d508-dd3a54c7b608"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 100 accuracy 0.96\n",
            "batch 200 accuracy 0.92\n",
            "batch 300 accuracy 0.96\n",
            "batch 400 accuracy 0.98\n",
            "batch 500 accuracy 0.94\n",
            "batch 600 accuracy 0.94\n",
            "batch 700 accuracy 0.98\n",
            "batch 800 accuracy 0.96\n",
            "batch 900 accuracy 0.92\n",
            "batch 1000 accuracy 0.96\n",
            "batch 1100 accuracy 0.9\n",
            "batch 1200 accuracy 0.94\n",
            "accuracy of entire set 0.945\n"
          ]
        }
      ]
    }
  ]
}